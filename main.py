#!/usr/bin/env python3
"""
LLM Evaluations - Main entry point for evaluating LLM solutions against benchmark dataset

This tool evaluates solutions generated by language models against a benchmark dataset
of programming problems. It produces comprehensive metrics including overall scores,
category-based performance, complexity analysis, and error reporting.
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import time
from collections import defaultdict
import logging

# Establish ROOT_DIR based on the location of this main.py file
ROOT_DIR = Path(__file__).parent

from evaluator.core_evaluator import CoreEvaluator
from evaluator.metrics_calculator import MetricsCalculator
from evaluator.error_analyzer import ErrorAnalyzer
from evaluator.report_generator import ReportGenerator
from utils.jsonl_reader import JSONLReader
from utils.validation import validate_solution_format, validate_benchmark_format


def setup_logging(verbose: bool = False):
    """Setup logging configuration"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def load_benchmark_data(benchmark_dir: Path) -> Dict[str, Any]:
    """Load all benchmark problems from directory"""
    logger = logging.getLogger(__name__)
    logger.info(f"Loading benchmark data from: {benchmark_dir}")
    
    benchmark_data = {}
    total_problems = 0
    
    # Load all JSONL files from benchmark directory
    for jsonl_file in benchmark_dir.glob("*.jsonl"):
        category = jsonl_file.stem
        logger.debug(f"Loading category: {category}")
        
        problems = JSONLReader.read_file(jsonl_file)
        for problem in problems:
            if validate_benchmark_format(problem):
                task_id = problem['task_id']
                benchmark_data[task_id] = problem
                total_problems += 1
            else:
                logger.warning(f"Invalid benchmark format for task: {problem.get('task_id', 'unknown')}")
    
    logger.info(f"Loaded {total_problems} benchmark problems from {len(set(p['topic'] for p in benchmark_data.values()))} categories")
    return benchmark_data


def load_solution_data(solution_dir: Path) -> Tuple[Dict[str, Any], str]:
    """Load all solutions from directory and extract model name"""
    logger = logging.getLogger(__name__)
    logger.info(f"Loading solution data from: {solution_dir}")
    
    solution_data = {}
    model_name = None
    total_solutions = 0
    
    # Load all JSONL files from solution directory
    jsonl_files = list(solution_dir.glob("*.jsonl"))
    logger.debug(f"Found {len(jsonl_files)} JSONL files in {solution_dir}")
    
    for jsonl_file in jsonl_files:
        logger.debug(f"Loading solutions from: {jsonl_file}")
        
        solutions = JSONLReader.read_file(jsonl_file)
        for solution in solutions:
            if validate_solution_format(solution):
                task_id = solution['task_id']
                
                # Extract base task_id (remove model suffix if present)
                # Handle cases like "dataclass_1_gpt-4" -> "dataclass_1"
                base_task_id = task_id
                model_name_from_solution = solution.get('model', '')
                if model_name_from_solution and task_id.endswith(f'_{model_name_from_solution}'):
                    base_task_id = task_id[:-len(f'_{model_name_from_solution}')]
                
                # Use extracted_solution if available, otherwise candidate_solution
                solution_code = solution.get('extracted_solution', solution.get('candidate_solution', ''))
                solution_data[base_task_id] = {
                    **solution,
                    'candidate_solution': solution_code
                }
                
                # Extract model name from first solution
                if model_name is None and 'model' in solution:
                    model_name = solution['model']
                
                total_solutions += 1
            else:
                logger.warning(f"Invalid solution format for task: {solution.get('task_id', 'unknown')}")
    
    if model_name is None:
        # Try to extract from directory name - reconstruct full path if nested
        if solution_dir.parent.name != 'generated':
            # Handle nested directories like deepseek-ai/DeepSeek-R1-0528:novita
            model_name = f"{solution_dir.parent.name}/{solution_dir.name}"
        else:
            model_name = solution_dir.name
    
    logger.info(f"Loaded {total_solutions} solutions for model: {model_name}")
    return solution_data, model_name


def evaluate_solutions(benchmark_data: Dict[str, Any], 
                      solution_data: Dict[str, Any],
                      model_name: str,
                      output_dir: Path,
                      verbose: bool = False) -> Dict[str, Any]:
    """Evaluate solutions against benchmark and generate reports"""
    logger = logging.getLogger(__name__)
    logger.info(f"Starting evaluation for model: {model_name}")
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Initialize components
    evaluator = CoreEvaluator()
    metrics_calc = MetricsCalculator()
    error_analyzer = ErrorAnalyzer()
    report_gen = ReportGenerator()
    
    # Evaluate each solution
    evaluation_results = []
    absent_solutions = []
    
    for task_id, benchmark_problem in benchmark_data.items():
        if task_id in solution_data:
            # Evaluate the solution
            result = evaluator.evaluate_single_solution(
                benchmark_problem,
                solution_data[task_id]['candidate_solution'],
                task_id
            )
            result['model'] = model_name
            evaluation_results.append(result)
            
            if verbose and not result['all_tests_passed']:
                logger.debug(f"Task {task_id}: {result['tests_passed']}/{result['total_tests']} tests passed")
        else:
            # Track absent solution
            absent_solutions.append({
                'task_id': task_id,
                'category': benchmark_problem.get('topic', 'unknown'),
                'complexity': benchmark_problem.get('complexity', 0)
            })
    
    logger.info(f"Evaluated {len(evaluation_results)} solutions, {len(absent_solutions)} absent")
    
    # Calculate metrics
    metrics = metrics_calc.calculate_all_metrics(
        evaluation_results,
        absent_solutions,
        benchmark_data
    )
    
    # Analyze errors
    error_analysis = error_analyzer.analyze_errors(evaluation_results)
    
    # Generate reports
    report_gen.generate_evaluation_report(
        model_name,
        evaluation_results,
        metrics,
        error_analysis,
        output_dir
    )
    
    # Save detailed results
    results_file = output_dir / 'test_results.jsonl'
    with open(results_file, 'w') as f:
        for result in evaluation_results:
            f.write(json.dumps(result) + '\n')
    
    logger.info(f"Results saved to: {output_dir}")
    
    return {
        'model': model_name,
        'metrics': metrics,
        'error_analysis': error_analysis,
        'total_evaluated': len(evaluation_results),
        'total_absent': len(absent_solutions),
        'output_dir': str(output_dir)
    }


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Evaluate LLM-generated solutions against benchmark dataset',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Evaluate a single model (saves to generated/evaluation_timestamp/model/)
  python main.py --benchmark ../dataset/generated --solutions ../llm_solutions/generated/session_123/gpt-4
  
  # Evaluate with custom output directory
  python main.py --benchmark ../dataset/generated --solutions ../llm_solutions/generated/session_123/gpt-4 --output my_results
  
  # Verbose mode for debugging
  python main.py --benchmark ../dataset/generated --solutions ../llm_solutions/generated/session_123/gpt-4 --verbose
  
  # Evaluate multiple models (saves to generated/evaluation_timestamp/model1/, model2/, comparison.json)
  python main.py --benchmark ../dataset/generated --solutions ../llm_solutions/generated/session_123/gpt-4 ../llm_solutions/generated/session_123/claude-3
"""
    )
    
    parser.add_argument(
        '--benchmark',
        type=Path,
        required=True,
        help='Path to benchmark dataset directory containing JSONL files'
    )
    
    parser.add_argument(
        '--solutions',
        type=Path,
        nargs='+',
        required=True,
        help='Path(s) to solution directory/directories containing JSONL files'
    )
    
    parser.add_argument(
        '--output',
        type=Path,
        default=Path('generated'),
        help='Output directory for evaluation results (default: generated)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose output'
    )
    
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging(args.verbose)
    
    # Resolve benchmark path relative to ROOT_DIR
    if args.benchmark.is_absolute():
        benchmark_path = args.benchmark
    else:
        benchmark_path = ROOT_DIR / args.benchmark
    
    # Validate paths
    if not benchmark_path.exists():
        logger.error(f"Benchmark directory not found: {benchmark_path}")
        sys.exit(1)
    
    for solution_path in args.solutions:
        # Check if path exists relative to ROOT_DIR for relative paths
        if solution_path.is_absolute():
            check_path = solution_path
        else:
            check_path = ROOT_DIR / solution_path
        if not check_path.exists():
            logger.error(f"Solution directory not found: {check_path}")
            sys.exit(1)
    
    # Load benchmark data once
    benchmark_data = load_benchmark_data(benchmark_path)
    
    if not benchmark_data:
        logger.error("No valid benchmark problems found")
        sys.exit(1)
    
    # Create session ID for this evaluation run
    session_id = f"evaluation_{int(time.time())}"
    
    # Evaluate each solution directory
    all_results = []
    
    for solution_path in args.solutions:
        logger.info(f"\n{'='*60}")
        logger.info(f"Processing solutions from: {solution_path}")
        logger.info(f"{'='*60}")
        
        # Load solutions - resolve path relative to ROOT_DIR to avoid working directory issues
        if solution_path.is_absolute():
            absolute_solution_path = solution_path
        else:
            absolute_solution_path = ROOT_DIR / solution_path
        solution_data, model_name = load_solution_data(absolute_solution_path)
        
        if not solution_data:
            logger.warning(f"No valid solutions found in: {solution_path}")
            continue
        
        
        # Create model-specific output directory with session ID  
        if args.output.is_absolute():
            output_base = args.output
        else:
            output_base = ROOT_DIR / args.output
        model_output_dir = output_base / session_id / model_name
        logger.debug(f"Output directory path: {model_output_dir}")
        logger.debug(f"Output directory exists: {model_output_dir.exists()}")
        logger.debug(f"Parent directory exists: {model_output_dir.parent.exists()}")
        
        # Evaluate
        result = evaluate_solutions(
            benchmark_data,
            solution_data,
            model_name,
            model_output_dir,
            args.verbose
        )
        
        all_results.append(result)
    
    # Generate comparative report if multiple models
    if len(all_results) > 1:
        logger.info("\nGenerating comparative analysis...")
        report_gen = ReportGenerator()
        if args.output.is_absolute():
            session_output_dir = args.output / session_id
        else:
            session_output_dir = ROOT_DIR / args.output / session_id
        report_gen.generate_comparative_report(all_results, session_output_dir)
    
    # Print summary
    logger.info(f"\n{'='*60}")
    logger.info("EVALUATION SUMMARY")
    logger.info(f"{'='*60}")
    
    for result in all_results:
        metrics = result['metrics']
        logger.info(f"\nModel: {result['model']}")
        
        # Extract score from new structure
        avg_score = metrics['average_score']
        if isinstance(avg_score, dict):
            score = avg_score['score']
            ci = avg_score['confidence_interval']
            logger.info(f"  Total Score: {score:.2f}% ± {ci['margin_of_error']:.2f}% (95% CI: {ci['lower_bound']:.2f}%-{ci['upper_bound']:.2f}%)")
        else:
            # Backward compatibility
            logger.info(f"  Total Score: {avg_score:.2f}%")
            
        logger.info(f"  Solutions Evaluated: {result['total_evaluated']}")
        logger.info(f"  Solutions Absent: {result['total_absent']}")
        logger.info(f"  Results saved to: {result['output_dir']}")
    
    logger.info("\nEvaluation complete!")


if __name__ == '__main__':
    main()